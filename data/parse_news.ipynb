{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "680de52d",
   "metadata": {},
   "source": [
    "*Ensure all required libraries for scraping are installed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "97a09e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "27e52a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "THE_BATCH_URL = 'https://www.deeplearning.ai/the-batch/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f714e65e",
   "metadata": {},
   "source": [
    "### Option 1: Scrape using urls from sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "695eb009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SITEMAP_URL_0 = 'https://www.deeplearning.ai/sitemap-0.xml'\n",
    "# SITEMAP_URL_1 = 'https://www.deeplearning.ai/sitemap-1.xml'\n",
    "\n",
    "# def get_sitemap_urls(sitemap_url):\n",
    "#     response = requests.get(sitemap_url)\n",
    "#     soup = BeautifulSoup(response.content, 'xml')\n",
    "#     urls = [url.text for url in soup.find_all('loc') if url.text.startswith(THE_BATCH_URL)]\n",
    "#     return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc798e8",
   "metadata": {},
   "source": [
    "### Option 2: Scrape only `the-batch/issue-x/`\n",
    "\n",
    "I noticed that every article tag is already included in the weekly issues, so there's probably no need to scrape each article individually.\n",
    "\n",
    "\n",
    "For now, I'll use only the articles from **weekly issues**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9d4ab6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'articles/'\n",
    "image_output_dir = 'images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bb47324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url, image_name, images_dir):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            image_path = os.path.join(images_dir, image_name)\n",
    "            with open(image_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            return image_path\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "06da16fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Research whether this kind of cleaning is needed and if so, how to do it properly\n",
    "def clean_text(text: str):\n",
    "    cleaned_text = re.sub(r'[ \\t]+', ' ', text)                    # Remove extra spaces and tabs\n",
    "    cleaned_text = re.sub(r'\\n+', '\\n', cleaned_text)              # Remove extra newlines\n",
    "    \n",
    "    to_remove = [\n",
    "        '\\n Andrew\\n',\n",
    "        'Dear friends,\\n ',\n",
    "        'AndrewÂ \\n \\n ',\n",
    "    ]\n",
    "    for i in to_remove:\n",
    "        cleaned_text = cleaned_text.replace(i, '')\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "13628d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_paragraph_after_deeplearningai(soup):\n",
    "    \"\"\"\n",
    "    Used to remove the paragraph that follows the header \"A message from deeplearning.ai\"\n",
    "    This is an ad for the deeplearning.ai courses.\n",
    "    \n",
    "    P.S. not sure if this is the best way to do it, but it works for now.\n",
    "    \"\"\"\n",
    "    \n",
    "    header = soup.find('h2', id='a-message-from-deeplearningai')\n",
    "    if header:\n",
    "        next_p = header.find_next_sibling('p')\n",
    "        if next_p:\n",
    "            next_p.decompose()\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c081bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_issue_articles(start_from: int, last_issue: int):\n",
    "    articles_data = []\n",
    "    \n",
    "    for i in range(start_from, last_issue + 1):\n",
    "        url = f'{THE_BATCH_URL}issue-{i}/'\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        article_tags = soup.find_all('article')\n",
    "\n",
    "        for tag in article_tags:\n",
    "            tag = remove_paragraph_after_deeplearningai(tag)\n",
    "            \n",
    "            paragraphs = tag.find_all('p')\n",
    "            body = '\\n '.join(p.get_text() for p in paragraphs)\n",
    "            body = clean_text(body)\n",
    "\n",
    "            image_urls = [\n",
    "                img['src'] for img in tag.find_all('img')\n",
    "                if img.get('src') and img['src'].lower().endswith(('.png', '.jpg', '.jpeg', '.svg')) and\n",
    "                'The-Batch-ads-and-exclusive-banners' not in img['src']  # exclude ads\n",
    "            ]\n",
    "            \n",
    "            images = []\n",
    "            for j, img_url in enumerate(image_urls):\n",
    "                ext = os.path.splitext(img_url)[1].split('?')[0]\n",
    "                image_name = f\"issue_{i}_{j}\" + ext\n",
    "                image_path = download_image(img_url, image_name, image_output_dir)\n",
    "                \n",
    "                if image_path:\n",
    "                    images.append(image_path)\n",
    "\n",
    "            articles_data.append({\n",
    "                'link': url,\n",
    "                'body': body,\n",
    "                'images': images,\n",
    "            })  \n",
    "\n",
    "    return articles_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c67b550e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles from issue 251 to 300 saved to articles_251_300.json\n"
     ]
    }
   ],
   "source": [
    "START_FROM_ISSUE = 251\n",
    "LAST_ISSUE = 300\n",
    "CHUNK_SIZE = 50\n",
    "\n",
    "for start in range(START_FROM_ISSUE, LAST_ISSUE + 1, CHUNK_SIZE):\n",
    "    end = min(start + CHUNK_SIZE - 1, LAST_ISSUE)\n",
    "    articles_chunk = get_issue_articles(start, end)\n",
    "    \n",
    "    filename = os.path.join(output_dir, f'articles_{start}_{end}.json')\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(articles_chunk, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f'Articles from issue {start} to {end} saved to articles_{start}_{end}.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
